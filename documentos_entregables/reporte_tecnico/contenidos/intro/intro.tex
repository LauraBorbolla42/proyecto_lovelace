%
% Capítulo de introducción.
% Proyecto Lovelace.
%

\chapter{Introducción}

A finales de los ochenta, el uso de las computadoras y el internet comenzó a
popularizarse; compañías ya establecidas, como aerolíneas y tiendas
departamentales, y comerciantes independientes vieron una oportunidad de
expandirse y el comercio en línea comenzó a tomar fuerza; sin embargo, casi
nadie previó el impacto y auge que iba a tener, por lo que la mayoría de los
sitios no se encontraban preparados para los ataques y robos de información.
Las principales emisoras de tarjetas (MasterCard y Visa) reportaron entre 1988 y
1998 pérdidas de 750 millones de dólares debidas a fraudes con
tarjetas bancarias: el crecimiento del comercio electrónico aunado a
sistemas débilmente protegidos dio lugar a un rápido crecimiento de los
fraudes relacionados con tarjetas bancarias; de hecho, para el año 2001,
según \cite{wallethub}, se tuvieron pérdidas de 1.7 miles de millones de
dólares y, para el siguiente año habían aumentado a 2.1 miles de millones de
dólares.

Las compañías emisoras de tarjetas comenzaron a proponer soluciones y,
a finales de 1999, Visa publicó un estándar de seguridad para quienes realizaban
transacciones en línea llamado \gls{gl:cisp}, este programa es el primer
precursor del estándar actual \gls{gl:pci}~\gls{gl:dss}. Lamentablemente, las
recomendaciones no estaban unificadas y había incosistencias entre ellas; fueron
pocas las compañías que pudieron satisfacer completamente alguno de los
estándares publicados. Fue hasta finales de 2004 cuando se publicó el primer
estándar unificado, respaldado por las compañías emisoras de tarjetas más
importantes: el \gls{gl:pci} \gls{gl:dss} 1.0, este estándar indica a los
comercios qué es lo que hay que hacer para mantener los datos bancarios seguros
mediante protocolos de seguridad, y se hizo obligatorio para todos aquellos que
realizaran más de 20,000 transacciones anuales; aunque cada vez más compañías
comenzaron a seguirlo e invertir para satisfacer el estándar, de nuevo fueron
pocas las que alcanzaron a cumplirlo completamente, pues tiene una
gran cantidad de requerimientos (controles estrictos de acceso,
monitoreo regular de las redes, mantener programas de vulnerabilidaes y
políticas de seguridad de información, etcétera) y las compañías tendían a
subestimar los costos que implica seguir el
estándar~\cite{uk_association, search_security}.

A finales del 2006 ocurrió una de las primeras grandes violaciones de datos
que puso en guardia a todos: hubo una intrusión en los servidores de la empresa
americana TJX y piratas cibernéticos robaron información de tarjetas de crédito,
débito y transacciones de 94 millones de clientes registrados en el sistema de
la empresa. Ataques como este han continuado a lo largo de los años: la cadena
de supermercados Hannaford Bros. sufrió una violación en 2008 y se vieron
comprometidas 4.2 millones de cuentas, Target fue atacado en 2013 y 40 millones
de cuentas fueron afectadas, y, al año siguiente la seguridad de Home Depot
también fue violada y 56 millones de cuentas fueron afectadas tras el ataque.

Durante la primera década del siglo XXI, el enfoque que se tenía era
salvaguardar la información sensible en todo el sistema; tómese por ejemplo el
caso del sistema de una tienda en línea: el número de tarjeta queda registrado
en el área de clientes, pues se puede asociar con un perfil y evita tener que
estar ingresando continuamente toda la información de la tarjeta; también queda
registrada en el área de ventas, pues queda asociada a una compra o transacción;
la información sensible parece estar en todos lados (al menos, no está
concentrada) y tener que protegerla constantemente resulta muy costoso. Pasados
unos años, surge la idea de cambiar completamente el paradigma que se había
estado utilizando hasta ahora: ¿por qué intentar proteger la información
sensible en todos los lados donde sea que se encuentre, cuando se puede cambiar
esa información por un valor sustituto y solo proteger esa pequeña parte del
sistema? Así que, a mediados del 2011, el \gls{gl:pci}~\gls{gl:dss} publicó
las guías de tokenización; a su vez, compañías comenzaron a ofrecer soluciones
de tokenización que quitaban casi por completo el peso de cumplir con el
\gls{gl:pci}~\gls{gl:dss} a los comerciantes, pues ellas se encargan de generar
los \glspl{gl:token} y almacenar los datos sensibles; mientras que ellos,
los comerciantes, se quedan solo con el \gls{gl:token}; así, si la seguridad de
su sitio es violada, los datos siguen estando seguros, pues no se puede robar
lo que no existe dentro del sistema.

Aunque en este trabajo se hace especial referencia a la tokenización de los
números de tarjetas, o \gls{gl:pan}, este proceso sirve para proteger otros
datos, como números de seguridad social, claves de registro, números de
teléfono, etcétera. De hecho, la tokenización no está limitada al mundo digital
y ha estado presente desde hace mucho tiempo, por ejemplo, los billetes y
monedas, o las fichas en un casino: uno deposita dinero y obtiene su
equivalente en fichas (este proceso es el de tokenización) para jugar; aunque
las fichas (o \glspl{gl:token}) están vigiladas, si alguien las roba, el casino
no pierde tanto (siempre y cuando no sean cambiadas por dinero, o sea, realizar
el proceso de detokenización), pues es una mera representación, y no el dinero
mismo. Lo mismo sucede con los \glspl{gl:token} digitales: sustituyen
información valiosa, por valores que carecen de significado y que cuya pérdida
no representa un peligro inminente o una violación de la privacidad.

Todos los métodos para generar \glspl{gl:token} que se presentan en este trabajo
(y la gran mayoría de los métodos conocidos) competen enteramente a la
criptografía. Entre otras cosas, la criptografía permite proteger información de
terceros no autorizados, esto es, permite obtener confidencialidad. La idea
básica es transformar un mensaje de forma que solo el destinatario sea capaz
de hacer la transformación inversa y leer el mensaje original. Aunque con
métodos un poco distintos a los actuales, la criptografía tiene una larga
historia: el uso más antiguo del que se tiene noticia es en jeroglíficos
egipcios de alrededor de 1900 a. C.; a partir de ahí, hay evidencias de su uso
en muchas otras culturas antiguas~\cite{codebreakers}. La historia de la
criptografía moderna está muy relacionada con la historia de la computación: en
muchas ocasiones, la principal motivación para el desarrollo de máquinas más
potentes fue la posibilidad de crear un método de cifrado infalible (o del
método para romperlo); el ejemplo más claro de esto es el desarrollo de
métodos para descifrar los mensajes de los nazis, durante la segunda guerra
mundial, lo que permitió a Alan Turing sentar las bases de la computación
actual~\cite{simon_singht}. Esta dinámica de juego, en la cuál cada participante
está siempre buscando un método que los adversarios no pueden romper,
hace de la criptografía una ciencia en constante movimiento: una vez que
se encuentra una nueva vulnerabilidad en algún método, se trabaja para
corregirlo o para reemplazarlo.

Uno de los aspectos más importantes de las herramientas criptográficas
modernas es que buscan probar, dentro de los límites posibles, la
seguridad de un algoritmo. Es por esto que hoy en día no basta con
describir un nuevo método y esperar que sea lo suficientemente fuerte,
sino que la presentación de un método debe de ir acompañada de los
argumentos (las pruebas) que garantizan su seguridad. Hacer esto no es
tarea sencilla (si lo fuera, el juego ya estaría ganado para uno de los
bandos) pues involucra hacer suposiciones sobre las capacidades de
los adversarios y, basándose en esto, garantizar que los recursos necesarios
para romper el método probado se encuentran muy por encima de las capacidades
supuestas en un inicio. En este contexto, las capacidades de un adversario
se determinan por su poder de cómputo: ¿cuántas computadoras tiene a su
disposición?, ¿qué tan rápidas son?, etcétera.

Lo anterior nos lleva a otro principio muy importante de la criptografía
moderna: la seguridad se encuentra en los datos, no en el método. La historia
de la criptografía muestra que durante mucho tiempo, la seguridad de un método
consistía totalmente en mantenerlo en secreto: mientras los adversarios
no supieran cómo se estaba cifrando algo, los mensajes permanecerían seguros;
sin embargo, si el método se filtraba, todos los mensajes cifrados con ese
método se veían comprometidos. Este esquema presenta un serio problema,
pues hace dependiente de un solo secreto, el método, la seguridad de todos
los mensajes. Esto llevó a la introducción del concepto de la llave dentro
de la criptografía: un valor que solo conocen las entidades autorizadas a leer
el mensaje, sin el cuál no pueden descifrarlo; de esta forma la seguridad de un
mensaje se basa tanto en el conocimiento del método, como en el conocimiento
de la llave. En la mayoría de los métodos usados hoy en día, la seguridad se
basa enteramente en la llave, dejando el método abierto al escrutinio de todos.
Hacer esto presenta varias ventajas: una muestra de que el algoritmo está
bien diseñado y es lo suficientemente fuerte es que no necesita mantenerse en
secreto; cuando el algoritmo es público y es usado por muchas entidades (entre
más mejor) son mayores las posibilidades de encontrar vulnerabilidades y
el tiempo de respuesta de las correcciones es menor.

La presentación que se hace en este trabajo de los métodos de tokenización
tiene un enfoque totalmente criptográfico. De esta forma se busca demostrar
que la tokenización es una aplicación de la criptografía, no una alternativa
a esta. Es por esto que todo el capítulo del marco teórico está dedicado a las
ramas de la criptografía que se relacionan con los métodos para generar
\glspl{gl:token}.

\section{Objetivos}

\subsection{Objetivo general}

Implementar algoritmos criptográficos y no criptográficos para generar
\glspl{gl:token} con el propósito de proveer confidencialidad a los datos de las
tarjetas bancarias\footnote{Tomando en cuenta la clasificación propuesta por el
\gls{gl:pci} \gls{gl:ssc}}.

\subsection{Objetivos específicos}

\begin{itemize}
  \item Revisar diversos algoritmos para la generación de \glspl{gl:token}.
  \item Diseñar e implementar un servicio web que proporcione el servicio de
    generación de \glspl{gl:token} de tarjetas bancarias a, al menos, una tienda
    en línea.
  \item Implementar una tienda web que use el servicio de generación de
    \glspl{gl:token}.
\end{itemize}

\section{Justificación}

Dado que es un tema relativamente nuevo, la desinformación y la desconfianza
respecto a la generación de \glspl{gl:token} es latente aún, pues, por ejemplo,
aunque el \gls{gl:pci}~\gls{gl:dss} describe los requerimientos que debe cumplir
un sistema tokenizador, no indica qué hacer para satisfacerlos. Muchas empresas
se aprovechan de esta desinformación para promocionarse como sistema
tokenizador sin ser claras sobre sus métodos. A continuación se muestran las
principales deficiencias encontradas como resultado de una investigación
comparativa de las principales soluciones que existen en el mercado.

\begin{description}

  \item[Shift4]
    Esta compañía reclama el crédito de haber inventado el paradigma de
    la tokenización, denuncia que otras compañías y el mismo
    \gls{gl:pci}~\gls{gl:ssc} desvirtuaron el término; por esta razón, la
    compañía se refiere a su propio método cómo \textit{TrueTokenization}.
    Aunque mencionan en todas partes las supuestas ventajas de sus métodos,
    lo único que dicen con claridad sobre la generación de \glspl{gl:token} es
    que se trata de valores aleatorios, únicos y alfanuméricos. Esta descripción
    no representa en modo alguno una garantía sobre la seguridad del
    método: podrían estar utilizando métodos pseudoaleatorios que no son
    criptográficamente seguros; o cualquier otro método cuya seguridad no esté
    formalmente probada~\cite{shif4_uno, shif4_dos}.

  \item[Bluepay TokenShield]
    En este servicio, a pesar de que se ofrecen dos formas distintas de
    tokenizar, nunca se aclara cómo es que funciona alguno de estos procesos.
    Además, en \cite{bluepay_tokenshield} hay una breve descripción del flujo
    de datos del servicio, donde se dice que con el \gls{gl:token} se recupera
    y descifra la información bancaria sensible, lo que genera confusión, pues
    no se sabe si la tokenización se hace por medio de de cifrados que
    preservan el formato\footnote{Tipo de cifrado que permite que los mensajes
    ya cifrados se vean parecidos a los originales. Este tema se abarca en la
    sección \ref{sec:fpe}.}, o por medio de tablas que
    contienen pares \gls{gl:pan}-\gls{gl:token}.

  \item[Braintree]
    Compañía que ofrece distintos \gls{gl:sdk} (para dispositivos móviles y
    para desarrollo web) que permiten interactuar con sus propios servidores
    para procesar pagos con tarjetas de crédito. En~\cite{braintree_uno}
    dan una definición de \gls{gl:token} que no es compatible con el
    \gls{gl:pci}~\gls{gl:ssc}: la única forma de generar \glspl{gl:token},
    según su definición, es por métodos aleatorios. Aún aceptando
    esa definición, no se ofrecen mayores explicaciones: los clientes no
    saben de qué forma se están generando los valores aleatorios.

  \item[Merchant Link]
    La solución de tokenización de Merchant Link es llamada TransactionVault. Su
    página da tres puntos importantes respecto a la tokenización:
    \begin{itemize}
      \item Utilizan tecnología de la \textit{siguiente generación}.
      \item Los \glspl{gl:token} son asociados con una tarjeta y una
        transacción.
      \item Todos los \glspl{gl:token} generados tienen una longitud de 16
        dígitos y pasan los controles de validez de las tarjetas bancarias
        (por ejemplo, el algoritmo de Luhn).
      \end{itemize}
      Aunque informan al cliente que los \glspl{gl:token} creados preservan el
      formato en el tercer punto, no indican qué algoritmos son utilizados para
      generarlos; aunado a esto, el segundo punto tampoco aporta mucha
      información: sí, utilizan algoritmos reversibles, pues los
      \glspl{gl:token} quedan ligados a una tarjeta o a una transacción,
      pero siguen sin especificar sus métodos \cite{merchant_link}.

    \item[Jack Henry Card Processing Solution]
      La solución de tokenización ofrecida por Jack Henry Banks indica que se
      encarga de sustituir el número de tarjeta con un \gls{gl:token} numérico
      válido que pasa los controles de validez  de las tarjetas bancarias; sin
      embargo, no dice qué métodos utiliza para generar los \glspl{gl:token}.
      Indica también que parte de su plataforma está integrada con los servicios
      de tokenización provistos por \gls{gl:mdes} y \gls{gl:vts}; sin embargo,
      estos servicios tampoco indican cómo generan los tokens. Finalmente, no
      queda claro quién se encarga de generar los \glspl{gl:token}, ¿lo hace
      \gls{gl:mdes}, \gls{gl:vts} o Jack Henry?\cite{jack_henry, mdes_1, mdes_2}

    \item[Securosis]
      La publicación en \cite{securosis} tiene por objetivo esclarecer el papel
      de la tokenización. Explica de forma bastante clara las ventajas del
      paradigma y el flujo de datos entre tienda, sistema tokenizador y banco;
      sin embargo, al igual que las empresas anteriores, carece de una
      explicación sobre los propios \glspl{gl:token}, y algunas de las
      referencias hacia estos solo consigue confundir. Por ejemplo, presentan
      a la tokenización como una alternativa a la criptografía, hablando
      de los \glspl{gl:token} como valores aleatorios que nada tienen que
      ver con criptografía; después de todo, el problema de la generación
      de números pseudoaleatorios es también una rama de la criptografía.

    % Thales Tokenization.
    % Al parecer este si dice cómo es que funciona, usa FPE, mas especifico FF3
    % para tokenizar información basada en texto, y tablas de búsqueda para
    % información numérica de 9 a 19 dígitos.
    % Aquí está la mayoría de la información:
    % (no importa la calidad de las imágenes)
    %http://go.thalesesecurity.com/rs/480-LWA-970/images/Fortrex_Vormetric_VTS_Token_Evaluation.PDF?aliId=59031373

\end{description}

Muchas de las soluciones del mercado intentan argumentar que, como para ellos
un \gls{gl:token} es un valor aleatorio, nada tiene que ver con criptografía;
al parecer esta estrategia publicitaria tuvo mucho éxito, pues uno de los
mensajes más comunes es que la criptografía es cosa del pasado, la tokenización
permite sustituirla. Mientras esto puede ser cierto para los posibles clientes
de un sistema tokenizador (tiendas que procesan pagos en línea), es una mentira
manifiesta cuando se considera a todos los  participantes. Otro error es pensar
en la generación de números pseudoaleatorios como un tema no criptográfico;
podría ser considerado así si se tratara de generadores realmente aleatorios
(los cuales son raros y difíciles de  implementar), sin embargo, si se trata de
números pseudoaleatorios, la relación con la criptografía es directa.

La mayoría de las soluciones tratan a sus métodos como secretos de compañía: no
explican la manera en que generan los \glspl{gl:token}, por lo que esperan que
el trato entre cliente y proveedor esté basado en la confianza que tiene el
primero por el segundo. El modelo basado en la confianza es común al
funcionamiento de casi todas las transacciones monetarias hechas por internet:
dos partes que quieran realizar una transacción necesitan de un tercero (la
institución financiera) para llevarla a cabo. Un estado ideal eliminaría la
necesidad de la tercera parte (es lo que intentan hacer
\glspl{gl:criptomoneda} como \textit{bitcoin}~\cite{bitcoin}), sin embargo,
mientras este estado no se alcance, el modelo basado en la confianza es un mal
necesario, y como tal, debe ser reducido al máximo: un tercero proveedor del
servicio de tokenización debería ser muy claro sobre lo que hace para
tokenizar, permitiendo que sus clientes basen en esto su desición.

La desinformación que hay alrededor del tema de la tokenización es la principal
justificación para este trabajo: se busca implementar algoritmos públicos
y presentar los resultado, permitiendo a los clientes de sistemas tokenizadores
entender qué hay adentro de la caja negra. Parte de este combate a la
desinformación consiste en dejar bien clara la relación entre la tokenización y
la criptografía: la tokenización es una forma de criptografía, por lo que los
métodos de tokenización deben ser analizados con la misma formalidad con la que
se estudian todas las demás formas de cifrado.

\section{Organización del documento}

El capítulo \ref{sec:marco_teorico} provee un conjunto de resúmenes sobre los
temas necesarios para la comprensión de este trabajo; adentrándose en la
criptografía, sus objetivos y los métodos que emplea para la protección de
información, tales como los cifradores por bloque y las funciones hash.

En el capítulo \ref{sec:generacion_de_tokens} se describen algunos de los
estándares pertinentes a la generación de \glspl{gl:token}, así como las
descripciones de los algoritmos existentes para realizar esta labor.

El capítulo \ref{sec:analisis_y_disenio} muestra el análisis técnico para
la implementación del programa tokenizador; abordando sus requerimientos,
la selección de tecnologías, y el diseño del propio programa.

El capítulo \ref{sec:implementacion} contiene los fragmentos de código más
importantes para entender el funcionamiento tanto de los algoritmos de
tokenización como el programa de generación de \glspl{gl:token}; también
presenta los resultados de las comparaciones de desempeño entre los
distintos algoritmos tokenizadores.

Por último, en el capítulo \ref{sec:conclusiones} se concluyen los resultados
del trabajo, hablando sobre los avances que se hicieron y el trabajo que aún
falta por hacerse.
