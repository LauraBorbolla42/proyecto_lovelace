%
% Capítulo de introducción,
% reporte técnico.
%
% Proyecto Lovelace.
%

\capitulo{Introducción}{sec:introduccion}
{
  \epigrafe
  {%
    In the beginning the Universe was created. This has made a lot of people
    very angry and has been widely regarded as a bad move.%
  }
  {%
    The Hitchhiker's Guide to the Galaxy,\\
    \textsc{Douglas Adams}.%
  }
  \epigrafe
  {%
    Welcome to a new year at Hogwarts! Before we begin our banquet, I would
    like to say a few words. And here they are: Nitwit! Blubber!
    Oddment! Tweak!%
  }
  {%
    Harry Potter and the Sorcerer's Stone,\\
    \textsc{J. K. Rowling}.%
  }
}

\noindent
A finales de los ochenta, el uso de las computadoras y el internet comenzó a
popularizarse; compañías ya establecidas, como aerolíneas y tiendas
departamentales, y comerciantes independientes vieron una oportunidad de
expandirse y el comercio en línea comenzó a tomar fuerza; sin embargo, casi
nadie previó el impacto y auge que iba a tener, por lo que la mayoría de los
sitios no se encontraban preparados para los ataques y robos de información.
Las principales emisoras de tarjetas (MasterCard y Visa) reportaron entre 1988 y
1998 pérdidas de 750 millones de dólares debidas a fraudes con
tarjetas bancarias: el crecimiento del comercio electrónico aunado a
sistemas débilmente protegidos dio lugar a un rápido crecimiento de los
fraudes relacionados con tarjetas bancarias; de hecho, para el año 2001,
según \cite{wallethub}, se tuvieron pérdidas de 1.7 miles de millones de
dólares y, para el siguiente año habían aumentado a 2.1 miles de millones de
dólares.

Las compañías emisoras de tarjetas comenzaron a proponer soluciones y,
a finales de 1999, Visa publicó un documento con una serie de recomendaciones
de seguridad para quienes realizaban transacciones en línea llamado
\gls{gl:cisp}, este programa es el primer precursor del estándar actual
\gls{gl:pci}~\gls{gl:dss}. Lamentablemente, las recomendaciones no estaban
unificadas y había inconsistencias entre ellas, por lo que pocas compañías
fueron capaces de satisfacer completamente alguna de las normas publicadas.

Fue hasta finales de 2004 cuando se publicó el primer estándar unificado,
respaldado por las compañías emisoras de tarjetas más importantes: el
\gls{gl:pci}~\gls{gl:dss} 1.0, en el cuál se indica a los comercios cómo
mantener los datos bancarios seguros mediante protocolos de seguridad, y se hizo
obligatorio para todos aquellos que realizaran más de 20,000 transacciones
anuales. Aunque cada vez más compañías comenzaron a seguirlo e invertir para
satisfacerlo, de nuevo fueron pocas las que alcanzaron a cumplirlo
completamente, pues tiene una gran cantidad de requerimientos (controles
estrictos de acceso, monitoreo regular de las redes, mantener programas de
vulnerabilidades y políticas de seguridad de información, etcétera) y las
compañías tendían a subestimar los costos que implica seguir el
estándar~\cite{uk_association, search_security}.

A finales del 2006 ocurrió una de las primeras grandes violaciones de datos
que puso en guardia a todos: hubo una intrusión en los servidores de la empresa
americana TJX y piratas cibernéticos robaron información de tarjetas de crédito,
débito y transacciones de 94 millones de clientes registrados en su sistema.
Ataques como este han continuado a lo largo de los años: la cadena
de supermercados Hannaford Bros. sufrió un embate en 2008 y se vieron
comprometidas 4.2 millones de cuentas, Target fue atacado en 2013 y 40 millones
de cuentas fueron afectadas, y, al año siguiente, arremetieron contra Home Depot
y la información de 56 millones de usuarios fue robada.

Durante la primera década del siglo XXI, el enfoque que se tenía era
salvaguardar la información sensible en todo el sistema; tómese por ejemplo el
caso de una tienda en línea: el número de tarjeta queda registrado
en el área de clientes, pues se puede asociar con un perfil y evita tener que
estar ingresando continuamente toda la información de la tarjeta; también queda
registrada en el área de ventas, pues queda asociada a una compra o transacción;
la información sensible parece estar en todos lados (al menos, no está
concentrada) y tener que protegerla constantemente resulta muy costoso. Pasados
unos años, surge la idea de cambiar completamente el paradigma que se había
estado utilizando hasta ahora: ¿por qué intentar proteger la información
sensible en todos los lados donde sea que se encuentre, cuando se puede cambiar
esa información por un valor sustituto y solo proteger esa pequeña parte del
sistema? Así que, a mediados del 2011, el \gls{gl:pci}~\gls{gl:dss} publicó
un documento cuyo título en inglés es «\textit{PCI DSS tokenization
guidelines}»; a su vez, varias compañías comenzaron a ofrecer soluciones de
tokenización\footnotemark{} que quitaban casi por completo el peso de cumplir
con el \gls{gl:pci}~\gls{gl:dss} a los comerciantes, pues ellas se encargan de
generar los \glspl{gl:token} y almacenar los datos sensibles; mientras que
ellos, los comerciantes, se quedan solo con el \gls{gl:token}; así, si la
seguridad de su sitio es violada, los datos siguen estando seguros, pues no se
puede robar lo que no existe dentro del sistema.

%
% El grupo vacío después de «footnotemark» no es necesario por la sintaxis,
% sin embargo, si no se pone, LaTeX deja un espacio muy corto entre el número
% y la siguiente palabra. Extraño.
%
% https://tex.stackexchange.com/questions/42982/wrong-space-with-footnotemark
%

\footnotetext{
  El término \textit{tokenización} es un anglicismo y, en este documento,
  se refiere a la acción y efecto de \textit{tokenizar}; es decir, dado un
  valor, obtener un valor sustituto (\gls{gl:token}). La \textit{detokenización}
  es el proceso inverso: dado el valor sustituto (\gls{gl:token}), obtener el
  valor original. Estos préstamos lingüísticos se utilizan de manera constante
  a lo largo de este reporte.
}

Aunque en este trabajo se hace especial referencia a la tokenización de los
números de tarjetas, o \gls{gl:pan}, este proceso sirve para proteger otros
datos, como números de seguridad social, claves de registro, números de
teléfono, etcétera. De hecho, la tokenización no está limitada al mundo digital
y ha estado presente desde hace mucho tiempo, por ejemplo, los billetes y
monedas, o las fichas en un casino: uno deposita dinero y obtiene su
equivalente en fichas (este proceso es el de tokenización) para jugar; aunque
las fichas (o \glspl{gl:token}) están vigiladas, si alguien las roba, el casino
no pierde tanto (siempre y cuando no sean cambiadas por dinero, o sea, realizar
el proceso de detokenización), pues es una mera representación, y no el dinero
mismo. Lo mismo sucede con los \glspl{gl:token} digitales: sustituyen
información valiosa, por valores que carecen de significado y cuya pérdida
no representa un peligro inminente o una violación de la privacidad.

Todos los métodos para generar \glspl{gl:token} que se presentan en este trabajo
(y la gran mayoría de los métodos conocidos) competen enteramente a la
criptografía. Entre otras cosas, la criptografía permite proteger información de
terceros no autorizados, esto es, permite obtener confidencialidad. La idea
básica es transformar un mensaje de forma que solo el destinatario sea capaz
de hacer la transformación inversa y leer el mensaje original. Aunque con
métodos un poco distintos a los actuales, la criptografía tiene una larga
historia: el uso más antiguo del que se tiene noticia es en jeroglíficos
egipcios de alrededor de 1900 a. C.; a partir de ahí, hay evidencias de su uso
en muchas otras culturas antiguas~\cite{codebreakers}. La historia de la
criptografía moderna está muy relacionada con la historia de la computación: en
muchas ocasiones, la principal motivación para el desarrollo de máquinas más
potentes fue la posibilidad de crear un método de cifrado infalible (o del
método para romperlo); el ejemplo más claro de esto es el desarrollo de
métodos para descifrar los mensajes de los nazis, durante la segunda guerra
mundial, lo que permitió a Alan Turing sentar las bases de la computación
actual~\cite{simon_singht}. Esta dinámica de juego, en la cuál cada participante
está siempre buscando un método que los adversarios no pueden romper,
hace de la criptografía una ciencia en constante movimiento: una vez que
se encuentra una nueva vulnerabilidad en algún método, se trabaja para
corregirlo o para reemplazarlo.

Uno de los aspectos más importantes de las herramientas criptográficas
modernas es que buscan probar, dentro de los límites posibles, la
seguridad de un algoritmo. Es por esto que hoy en día no basta con
describir un nuevo método y esperar que sea lo suficientemente fuerte,
sino que la presentación de un método debe de ir acompañada de los
argumentos (las pruebas) que garantizan su seguridad. Hacer esto no es
tarea sencilla (si lo fuera, el juego ya estaría ganado para uno de los
bandos) pues involucra hacer suposiciones sobre las capacidades de
los adversarios y, basándose en esto, garantizar que los recursos necesarios
para romper el método probado se encuentran muy por encima de las capacidades
supuestas en un inicio. En este contexto, las capacidades de un adversario
se determinan por su poder de cómputo: ¿cuántas computadoras tiene a su
disposición?, ¿qué tan rápidas son?, etcétera.

Lo anterior nos lleva a otro principio muy importante de la criptografía
moderna: la seguridad se encuentra en los datos, no en el método. La historia
de la criptografía muestra que durante mucho tiempo, la seguridad de un método
consistía totalmente en mantenerlo en secreto: mientras los adversarios
no supieran cómo se estaba cifrando algo, los mensajes permanecerían seguros;
sin embargo, si el método se filtraba, todos los mensajes cifrados con ese
método se veían comprometidos. Este esquema presenta un serio problema,
pues hace dependiente de un solo secreto, el método, la seguridad de todos
los mensajes. Esto llevó a la introducción del concepto de la llave dentro
de la criptografía: un valor que solo conocen las entidades autorizadas a leer
el mensaje, sin el cuál no pueden descifrarlo; de esta forma la seguridad de un
mensaje se basa tanto en el conocimiento del método, como en el conocimiento
de la llave. En la mayoría de los métodos usados hoy en día, la seguridad se
basa enteramente en la llave, dejando el método abierto al escrutinio de todos.
Hacer esto presenta varias ventajas: una muestra de que el algoritmo está
bien diseñado y es lo suficientemente fuerte es que no necesita mantenerse en
secreto; cuando el algoritmo es público y es usado por muchas entidades (entre
más mejor) son mayores las posibilidades de encontrar vulnerabilidades y
el tiempo de respuesta de las correcciones es menor.

La presentación que se hace en este trabajo de los métodos de tokenización
tiene un enfoque totalmente criptográfico. De esta forma se busca demostrar
que la tokenización es una aplicación de la criptografía, no una alternativa
a esta. Es por esto que todo el capítulo del marco teórico está dedicado a los
mecanismos criptográficos que se relacionan con los métodos para generar
\glspl{gl:token}.

\subimport{/}{objetivos}
\subimport{/}{justificacion}
\subimport{/}{estructura}
